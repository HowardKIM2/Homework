* 메모리 계층 구조

속도 순으로

1. 레지스터
2. 캐시
3. 메모리
4. 디스크

용량 순으로

1. 디스크
2. 메모리
3. 캐시
4. 레지스터



* x86 범용 레지스터들

1. ax: 함수의 return 값을 저장함
2. cx: 무언가를 반복하고자 할 때 사용
3. bp: 스택의 기준점
4. sp: 스택의 최상위점
5. ip: 다음에 실행할 명령어의 주소



* 스택은 아래로 자란다.

값이 쌓이면 스택은 - 의 주소를 가지게 됨
반대로 값이 빠지면 + 하게됨

Stack(스택)을 제외하고는 나머지는 전부
정상적인 절차로 쌓이게됨
쌓이면 + 빠지면 -



* 스택 ?! 이딴걸 어디에 쓰지 ?

레지스터는 메모리 계층 구조를 봤을 때
용량은 작고 속도는 빠르다.
레지스터 수천만개를 쓰면 참 좋겠지만
단가가 너무 높아진다는 문제가 있다.
그렇다면 단가도 낮추고
용량 문제도 해결할 수 있으면 좋을텐데
마침 메모리라는 녀석이 있다.
레지스터가 부족하면 메모리에 값을 잠깐
저장했다가 필요하면 다시 레지스터로 불러쓰자!
라는 개념에서 만들게된다.
(물론 좀 더 엄밀하게는 함수 호출을 위한 용도임)

x86 의 경우엔 함수의 입력을 스택으로 전달함
ARM 의 경우엔 함수 입력을 4 개까진 레지스터로 처리
하고 4 개가 넘어갈 경우엔 스택에 집어넣음
(그러므로 성능을 높이고자 한다면
ARM 에서는 함수 입력을 4 개 이하로 만드는 것이 좋음)

* 추가 정보

우선 모든 프로세서는
레지스터 에서 레지스터로 연산이 가능하다.
x86 은 메모리 에서 메모리로 연산이 가능하다.
하지만 ARM 은 로드/스토어 아키텍처라고 해서
메모리 에서 메모리로 연산이 불가능하다.
(반도체 다이 사이즈가 작기 때문임)
다이 사이즈가 작아서 Functional Unit 갯수가 적음
(이런 연산을 지원해줄 장치가 적어서 안됨)

그래서 ARM 은 먼저 메모리에서 레지스터로 값을 옮기고
다시 이 레지스터 값을 메모리로 옮기는 작업을함
로드하고 스토어하는 방식이라고 해서
로드/스토어 아키텍처라고함



* 프로그램 디버깅 및 어셈블리 분석

우선 사용할 저장소를 다운받는다.
그전에 디렉토리를 먼저 만든다.

mkdir my_proj
cd my_proj

이후 git 으로 저장소를 받는다.

git clone https://github.com/SHL-Education/Homework.git

만약 이미 저장소를 clone 받았다면

cd Homework
git pull origin master

위와 같이 입력하면 갱신(업데이트)된 내용을 받아볼 수 있다.
pwd 를 입력하여 현재 위치를 확인하고 아래 위치로 이동한다.

cd ~/my_proj/Homework/sanghoonlee

디버깅 옵션을 집어넣어서 컴파일 한다.

gcc -g -o debug func1.c

정상적으로 컴파일 되었다면 ls 입력시 debug 가 보일 것이다.

./debug

위와 같이 입력하여 6 이 잘 출력되는지 확인한다.
잘 출력되었다면 프로그램에 이상이 없으므로
정상적인 어셈블리 분석이 가능함을 의미한다.

* Instruction Scheduling(명령어 스케쥴링)

요즘 gcc(컴파일러)가 너무 똑똑해져서
최적화 옵션을 주지 않아도 알아서 최적화해버리는데
강제로 최적화를 못하게 할 수 있다.

-O0 옵션을 추가하면 된다: 영문자 O 와 숫자 0 이다.

즉 위의 컴파일 옵션을 아래와 같이 바꾼다.

gcc -g -O0 -o debug func1.c

모든 최적화를 방지하고 디버깅 옵션을 집어넣었음을 의미한다.



* 왜 이 작업이 필요한가 ?

디버깅 작업시에 가장 큰 문제가
C 언어 소스 코드와 실제 CPU 의 동작 흐름이 일치하지 않으면
분석을 하기가 굉장히 어렵다.

명령어를 스케쥴링 한다는 것은 아래와 같은 의미다.
어떤 명령어를 처리하는데 10 clock 이 필요한 것이 있고
어떤 것은 5 clock 이 걸리고 어떤것은 1 clock 이 걸린다.
이 배치를 효율적으로 진행해서 가장 빠르게 실행될 수 있게
만들어주는 작업을 명령어 스케쥴링이라고 한다.
그래서 컴파일러의 최적화 옵션이 활성화되면
프로그램이 여기 갔다 저기 갔다 왔다 갔다 한다.
(정신이 없으니 분석하기가 매우 힘들다)

이러한 이유 때문에 디버깅 시에는
반드시 -O0 옵션을 주도록 한다.



* 이제 디버거를 켜보자!

gdb debug

이와 같이 입력하면 디버거가 켜지면서 (gdb) 창이 보일 것이다.
어제 작업했듯이 main 함수에 breakpoint(정지)를 걸어야 한다.

b main

위와 같이 입력하면 main 함수에서 멈춰달라는 뜻이다.
r 을 눌러서 프로그램을 구동하면 main 함수에 걸릴 것이다.
단 C 언어 기반으로 움직여서 선두의 어셈블리어를 지나치게 됨
우선 현재 어느 위치에 있는지 파악하기 위해
디스어셈블리를 수행하도록 한다.

disas

이 명령어를 입력하면 현재 프로그램에 대한
디스어셈블리된 어셈블리어 코드가 보인다.
그리고 화살표가 보일 것이다.
여기서 p/x $rip 라고 입력해보도록 한다.

아주 흥미롭게도 '=>' 가 나타내는 주소값과
p/x $rip 가 출력하는 값이 같을 것이다.
앞서서 레지스터를 설명할 때
ip 값은 다음에 실행할 명령어의 주소를 가르킨다고 했었다.
(이 내용이 증명 되었다)

어쨋든 실행을 맨 처음으로 돌려야 하므로
disas 해서 맨 첫 라인의 주소를 복사한다.
그리고 아래와 같이 breakpoint 를 걸어준다.

b *복사한주소

b 하고 띄어쓴 다음에 * 하고 복사한주소를 붙여쓴다.
그리고 다시 r 을 눌러서 실행하면
'=>' 가 맨 처음에 배치되어 있음을 볼 수 있을 것이다.
(이거 볼 땐 disas 를 입력함)



* 16 진수

숫자	16 진수
0	0
1	1
2	2
3	3
4	4
5	5
6	6
7	7
8	8
9	9
10	a
11	b
12	c
13	d
14	e
15	f

* 진수 시스템

16 진수 0 ~ f 까지 총 16 개 - 컴퓨터가 씀
10 진수는 0 ~ 9 까지 총 10 개 - 인간이 씀
8 진수는 0 ~ 7 까지 총 8 개 - 리눅스 권한에 사용
3 진수 0 ~ 2 까지 총 3 개 - RNA 분석에 사용됨
2 진수 0, 1 로 총 2 개 - 역시 컴퓨터가 씀

? 2 진수랑 16 진수의 목적이 일치하는 것 아니냐 ?
라는 질문이 나올 수 있음
0101010101	2 진수
0x155		16 진수

1010101010101010101010		2 진수
0x2aaaaa			16 진수

뭐가 보기 편한가요 ?

10101010101010010101010101010101001010101010101001010

보기만 해도 토할 것 같아요.
즉 컴퓨터가 기계어를 사용하면서도
인간이 상대적으로 쉽게 볼 수 있는
16 진수 시스템을 채택한 용도로서 16 진수가 사용됨
(컴퓨터는 2 진수만을 쓰는 것임)

즉, 16 진수의 목적은
컴퓨터를 배운 사람과 기계의 혼용어라고 보면됨



* 2 진수, 16 진수를 어떻게 변환하는가 ?

144 이녀석을 2 진수로 쉽게 만드는 방법
일단 자릿수를 생각해야 한다.
먼저 144 를 10 진수 개념에서 분해해보자!

1 x 10^2 + 4 x 10^1 + 4 x 10^0

가만 보면 10 진수 시스템에서는
곱하는 부분에 10 이 계속 곱해지고 있음을 볼 수 있다.
그리고 승수로 붙는 곳에 자릿수 - 1 이
배치되고 있음을 볼 수 있다.

그렇다면 2 진수도 비슷하게 생각해보면 되지 않을까 ?
먼저 2 진수의 자릿수를 쭉 적는다.

7	6	5	4	3	2	1	0
128	64	32	16	8	4	2	1
2^7	2^6	2^5	2^4	2^3	2^2	2^1	2^0

위와 같은 형식을 가제 한다면 아래와 같이 적으면 된다.
144 = 128 + 16 이다.

위 색인에서 7 번째에 1 을 셋팅하고
4 번째에 1 을 셋팅하면 아래와 같이 된다.

1001 0000

이것이 144 의 2 진수 변환에 해당한다.
그렇다면 정말로 이게 10 진수 144 가
맞는지 확인할 필요가 있다.
10 진수에 적용한대로 동일한 계산을 적용한다.

1 x 2^7 + 1 x 2^4 = 128 + 16 = 144

즉 10 진수 144 가 2 진수 1001 0000 과 같음이 입증되었다.
여기서 16 진수로 바꾸는 작업은 훨씬 쉽다.
16 진수는 한 자리에 16 개가 온다.

먼저 2 진수 자리 1 개를 생각해보자

0, 1				2 개

다음으로 2 진수 2 자리를 생각해본다.

00, 01, 10, 11			4 개

2 진수 3 자리

000, 001, 010, 011, 100, 101, 110, 111		8 개

2 진수 4 자리면 ?		16 개

그래서 16 진수 변환을 수행할 때 4 자리씩 끊어치면 빠르다.
1001 0000 은 결국 0x90 이 된다.
확인은 10 진수 분해와 동일하게 하면 된다.

16^1	16^0
9	0

16^1 x 9 = 144



ex) 10 진수 33 을 2 진수 및 16 진수로 표기해보자.

33 = 32 + 1

32	16	8	4	2	1
1	0	0	0	0	1

10 0001

8421	8421
0010	0001
------------
0x2	1

0x21 => 2 x 16^1 + 1 x 16^0 = 33

ex) 10 진수 2568 을 2 진수 및 16 진수로 표기해보자.

2^10 = 1024
2^11 = 2048

2048	1024	512	256	128	64	32	16   8   4   2   1
1	0	1	0	0	0	0	0    1   0   0   0

2568 - 2048 = 520
520 - 512 = 8
8 - 8 = 0

1010 0000 1000

	8421	8421	8421
	1010	0000	1000
----------------------------
0x	a	0	8

0xA08 => A x 16^2 + 8 x 16^0 = 256 x 10 + 1 x 8 = 2568

ex) 0x48932110 을 2 진수로 변환해보자.

16 진수 1 자리가 2 진수 4 자리라는 것을 기억하고 풀면됨

8421	8421	8421	8421	8421	8421	8421	8421
0100	1000	1001	0011	0010	0001	0001	0000

0100 1000 1001 0011 0010 0001 0001 0000(2)



* 단위: 비트

2 진수(0, 1)로 표기할 수 있는 하나의 단위
2 진수 1 자리가 결국 1 bit(비트)를 의미함
1 bit 가 8 개 모이면 8 bit 이것을 1 byte 라고함

2^10 byte = 1 KB
2^10 KB   = 1 MB
2^10 MB   = 1 GB

32 bit = 2^32 = 4GB = 2^10 x 2^10 x 2^10 x 2^2
                    = 2^(10 + 10 + 10 + 2)

0 ~ 0xffff ffff		16 진수 8 자리

16 진수는 16 개를 표현하므로 1 비트가 4 개 있으면됨
16 진수 1 자리는 결국 4 비트로 표현됨

32 비트는 몇 개의 16 진수 자리로 구성되는가 ?
32 / 4 = 8 개

0x~~~~~~~~~		64 비트는 16 개

64 비트에서 16 진수 16 개에 모두 f 를 채우면 표현할 수 있는 최대값
8 비트 시스템을 생각해서 문제를 좀 더 단순화시켜보자!

0 ~ 0xff

표현할 수 있는 갯수는 2^8 = 256

256   128   64   32   16   8   4   2   1
1      0    0    0     0   0   0   0   0

0x100 - 1 = 0xff



* 포인터의 크기는 ?

8 비트 시스템의 경우 1 byte
16 비트는 2 byte
32 비트는 4 byte
64 비트는 8 byte

- 왜 그럴까 ?

컴퓨터의 산술 연산이 ALU 에 의존적이기 때문이다.
ALU 의 연산은 범용 레지스터에 종속적이고
컴퓨터가 64 비트라는 의미는 이들이 64 비트로 구성되었음을 의미한다.

변수의 정의는 메모리에 정보를 저장하는 공간이였다.
포인터의 정의는 메모리에 주소를 저장하는 공간이다.
그렇다면 64 비트로 표현할 수 있는 최대값 또한 저장할 수 있어야한다.
포인터의 크기가 작다면 이 주소를 표현할 방법이 없기 때문에
최대치인 64 비트(8 byte) 가 포인터의 크기가 된 것이다.

실제로 확인을 해보자!
지금 띄워놓은 터미널창을 우클릭한다.
New Terminal 이 보일텐데 클릭하면 새로운 터미널이 나타난다.

vi pointer_size.c

#include <stdio.h>
 
int main(void) {
	printf("sizeof(int *) = %lu\n", sizeof(int *));
	printf("sizeof(double *) = %lu\n", sizeof(double *));
	printf("sizeof(float *) = %lu\n", sizeof(float *));
	return 0;
}

결과가 전부 8 이 나오는 것을 볼 수 있을 것이다.

그러면 ? 아니 막말로 왜 이렇게 x 되게 빡센걸 하냐 ?
스택의 동작 과정이 포인터 베이스이기 때문임
또한 모든 컴퓨터의 동작과정이 이 포인터 베이스로 동작하게됨



앞서 수행했던 push rbp 쪽에 화살표를 오게하는 일련의 과정을 진행한다.
push 명령어는 현재 sp 에 뒤에 오는 값을 집어넣는 명령이다.
그러므로 Stack 이 증가하게 된다(주소값은 감소함)
그리고 이 Stack 안에는 rbp 의 값이 들어 있어야 한다.
이를 확인해보자!

우선 현재 화살표가 push %rbp 쪽에 있다.
그리고 p/x $rbp 와 p/x $rsp 를 입력해서 rbp 와 rsp 값을 기록한다.
그리고 si 를 입력해서 명령어 한 줄을 실행한다.
다음으로 다시 p/x $rsp 를 입력해서
이전 rsp 값에서 8 이 빠진것을 확인한다.
그리고 또한 이 rsp 의 주소 안에 값이 들어갔으므로
이를 확인하기 위해 x 명령어를 사용한다.
(x 명령어는 메모리 주소 내부에 있는 값을 보는 명령어다)
아래와 같이 입력한다.

x $rsp

여기서 앞서 기록했던 rbp 의 값이 보이면 잘 수행된 것이다.
결국 위에서 설명한 컴퓨터의 동작이 포인터 베이스(8 byte) 기준으로
동작함이 여기에서 입증된다.






















